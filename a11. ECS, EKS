Now imagine you hhave to run all your services in EC2 containers aka virtual machine each running their own OS and thus OS takes lot of resources and it is not optimal way. Thus, we can use docker which sits on top of OS and on top of docker engine we run our containers which are small and concise. Thus, all containers run isloated services sharing one OS and still having virtualization.

ECS (Elastic container service)
-------------------------------
used for managing containers in AWS. See slide 457 => It runs across multiple availability zones for high availability and thus it is region service. We create a ECS cluster which has a ECS service sitting in it and this ECS service is a way that we can specify the number of containers we want to run at any time. So, it's essentially a form of auto scaling for containers. Now inside this ECS service we have ECS container instances which are nothing but EC2 instances running in different AZ and these instances can again be managed using autoscaling group on top of them. On these EC2 instances we have ECS agent running which actually manages these instance from ECS perspective. And then inside these ec2 instances, we run the docker containers. These containers in ECS terms are called tasks and each task is told what image to run, how to run on cluster etc using the task definition which you can see in slide 457 on left having things like cpu, memory of the ec2 it will take, image to run (which we can get from dockerhub registry or amazon's own registry called ECR Amazon Elastic Container Registry), ports, etc. 

Launch types in ECS:
1. Serverless/ AWS Fargate => In this case we dont see the container instances or ASG groups for them anywhere and they are managed by AWS. We wont even see the EC2 instances in our EC2 console. ECS service is optional (serverless aspect of it i.e. just tell what container to run and AWS will make sure that service is fault tolerant, available and scales as per need without you worrying about capacity management). In terms of cost, we are paying per task.
EFS integration with the Fargate cluster is possible, but you don't get integration with Docker volumes or the FSx for Windows File Server which is possible in EC2 launch type.

2. Self Managed ECS/ EC2 launch type => In this, we have to create the container instances, the ASG groups for them for scaling, have full control over operations, have to manage the scaling of tasks as well using ECS service, have to install ECS agent on instances so they can be part of the cluster that ECS orchestration will use to bring up tasks specified using ECS service, patch management, operational control, security hardening, cluster optimization which is big thing (i.e. capacity management) are all of our responsibility. In terms of cost we are only paying for the EC2 instances and thus payment is per instance.
This supports diff storage services like docker volumes which can be mounted to these running containers and those volumes tied to services like EBS, EFS, FSX

Benefits of ECS: 
=> Offers serverless launching using AWS fargate
=> It is fully managed service whether serverless or not i.e. we don't have to build or manage a control plane or manage orchestration like kbs
=> ECS has full support for Docker containers, and you can also use the Docker Compose CLI as well.
=> You can fully integrate with Elastic Load Balancers so you can distribute the traffic that comes into your application just as you would with EC2. And the support extends to the ALB and the NLB.
=> ECS is not just about running Linux containers, it also has Windows container support as well.
=> Amazon ECS Anywhere is new feature to use for control plane of ECS i.e. its orchestration to your on-premises container implementations

IAM permissions:
let's see how to assign permissions to your containers/tasks to do things or to the EC2 instance running them i.e. ECS instance (only in case of self managed) => 
=> In case of EC2 launch type, we can have IAM permissions at 1) EC2 instance level called IAM instance role or 2) at the task level i.e. IAM Task role. The permissions in the EC2 instance role automatically applies to takss running inside that instance and thus be careful that all running containers will get these permissions. Task role can be used to give permission to container itself and other running container wont get those.
=> In case fargate => we just have IAM task roles since instances we dont manage => Task roles are defined in task definition.

Scaling ECS:
There are two different types of scaling: 
1. Service autoscaling for tasks which can change the desired number of tasks based on policies of resources like step policy or target policy or scheduling policy. The individual containers send the metrics to cloudwatch on their resource usage and that is used by above policies to act for scaling actions. 
2. Cluster autoscaling (for non-fargate one only) => In this we have capacity provider which collects the info from all the ec2 instances of resources usages and send them to ASG autoscaling group through cloudatch to scale the number of ec2 instances up or down. Once more instances are added, ECS service comes to know of the new node in cluster and startd bringing task on this new node. In case of scale down, ASG follows instance termination protection meaning, it is aware of the tasks running on the node and wont kill it. 

Amazon ECS with ALB: 
See slide 473 => We have multiple availability zones with private and public subnets. In the private subnets we got one container instance each and both are running 2 containers one nginx and one apache. On top of them we have two ECS services, and each service is running the Nginx and Apache containers resp.
So, as you can see, Nginx and Apache are both web services and they both run on port 80.
So, we have multiple containers on each container instance running on port 80 but that is not allowed as we can't have multiple port 80s used on single EC2 instance as instance wont know which container to forward incoming traffic at port 80. To solve this we have a dynamic port allocated on instance to our containers like show i.e. 32612 and 32600 => Now with ALB, we can make it aware of these dynamic ports and it can receive all 80 port traffic and being aware of traffic where to go, it can send it to correct dynamic port and host is aware of dyamic post and can run diff dynamic ports comfortably and forward it to correct container. 
Now, it's worth noting that if we have our containers running in a private subnet, we should have a NAT gateway in a public subnet and an entry in the route table for the private subnet, for any internet access that those containers might need.

Lab for Fargate launch: ECS service => clusters => create => networking only => enable container insights for cloudwatch capturing => create cluster => view the cluster => there is service tab for ECS service to scale tasks etc, tasks tab to add new containers you want to run, ECS instances but it wont be valid in fargate since AWS will do on its own, etc. In tasks => run new task => create new task definition => fargate => name, allocate a task memory and CPU which is fixed size and when load is like more than 80% (or some fargate rule) of this fixed size container resources, fargate will scale this task => thats why its necessary to give in fargate => add container => image = nginx, port 80 and add => create => go to tasks tab now => add new task => select nginx task definition, desired =1, choose web access security group, create and run => Now go to task id and networking tab inside task and public IP and open it => you should see nginx server.
Now go to task and stop it which will also delete it.

Lab for EC2 launch type cluster => Go to ECS cluster page and it already has our fargate cluster => create new cluster => Ec2 linux + networking => name, on demand instances, t2 micro, choose key pair for ssh, choose our VPC we created and choose 2 subnets of diff AZ => enabled auto assign public IP, public web security group, => instance IAM role, we have to create a role for instance (and tasks will auto get this role) to be able to talk to ECS service which is managing tasks scaling => choose create new role =>  enable insights => create > cloudformation (which we will stufy later) will create this cluster for us => go to cluster => new task definition => for ec2 => name, task role is role for the container itself => leave it for now => add container definition => image will be 'httpd:2.4' => port 80 and 80, hard limit of memory 32, create => now we go to cluster and ECS instance and we should see instance running of t2 micro type we chose => Go to service tab and create ECS service => EC2 => choose task definition => numnber of tasks 2, deployment type rolling update, next next next create => once this is done, we should see one task going from pending to running but why did other task not run since we specified in service for 2 tasks => 
See the events tab and it says that port on host instance is already occupoed which this is task 2 is trying to ask and this is actually used by task 1 because both task uses same definition which has port 80 specified => we can change the network mode for this and it will work => go to task definition and edit => network mode as bridge and it will make sure that all tasks it creates runs on diff port => in container config change ports to 0, 80 => Now go to service edit => and change the revision of task definition to this new definition. Now you should se both tasks coming up. 
Terminate the service which will also delete containers, drain the ec2 instances, delete the cluster finally. 

Lab using ALB with fargate => Now open the nginx task definition and check network mode is awsvpc which allows to run multiple containers with same port on same host. click fargate cluster => deploy => service, name, tasks = 4, load balancing ALB, port 80 http, target group will be tg-ecs-app2, health check grace 30 =:> networking => select subnets public one for 2 AZ, security group public web, assign public IP => you can see load balancer running, will see the task = 4 start to run and target group created automatucally.
Now go to cluster and service tab, select it and update => step 3 shows autoscaling settings for service => we can set min max task along with desired task as well => min max are used for autoscaling policy => add scaling policy => we can choose taget treacking policy (if 40 cpu to 80 and target was 60, launch 1 task) or step scaling (if 40 cpu to 80 directly, launch 2 tasks more) => You can also go to cloudwatch and choose container insights which will shows diff components resource usage and we can set alarms on them.
Delete service, delete load balancer, 

EKS (Elastic kubernetes service)
--------------------------------
Now EKS is like ECS but just that in EKS people use their own third party orchestration which is kubernetes maybe because they are comfortable with it or maybe because they dont wont to be locked down with AWS specific service in case they migrate in future. 

This can become necessary when let say you have orchestration on your on-premises and you want same orchestration to extend on AWS. Since you cant use ECS on-premises, but you can use EKS both places. This is calle hybrid deployment.
