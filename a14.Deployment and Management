AWS CloudFormation
-------------------
Cloudformation is a tool that we can use to build infrastructurem as a code i.e. we can write a template in yaml or json and cloudformation can use that template to create, modify or delete resources like ec2, vpc, subnet, etc in our AWS account based on template.
The benefits of using templates are: 1) Reduces time when deploying similar infras or same infra in diff regions as we can reuse the template 2)
Provisioned with less mistakes in config 3) Free to use 4) Peer review of template code possible 5) Easy rollback, update etc of the stack deployed using template. 
The diff components of cloudformation are: Stack which is actual env that cloudformation create using template, stack set when you want to deploy stack to set of regions/ multiple accounts etc together in one go, change set which shows the changes in stack when updating a template. 

Lab: Go to us-east region => cloudformation => create stack => template is ready, upload template, upload from folder 1 ec2 instance file which is self explanatory => next, name, IAM permission if not specified for cloudformation, it will use permission of current account => next, create and you will ec2 instance being created. (Can also this happening in cloudformation logs).  Let's update this stack now using file 2 under same folder which has just extra elastic IP attached to this ec2 instance => select stack in cloudformation => stack actions, create change set, replce template, upload, create create
See the file3 now => in there we can see that we can create KV type maps called mappings and then use them while defining resources => in resources we are asking to launch ec2 with image as ImageId: !FindInMap [AMIMap, !Ref "AWS::Region", dev] => means that find in mappings, AMI Map and in that, pick the key = AWS region from where this template is being run, and withing that dev key. Thus lets go to us west 1 => create stack, choose template and upload this => create => you will see in ec2 for us west 1 that instance will be launched with config we gave i.e. one instance only in same region for dev.
See file 4 now which shows use of parameter (LIKE ABOVE THIS IS IMP FOR EXAM) => Create stack, upload this tempalte, and in next step we will find parameter we provided in the template as option to choose from.
Delete all the stacks now. 

Lab: See 5th file i.e. ALBTemplate... => This will create multi AZ ASG and load balancer in front of it and other resouces like VPC => Go through the diff things in this file, => go to cloudformation, new stack, upload the template from s3 as that has latest verions of template => Choose instance type which is coming from parameter list, key value pair, name, subnet, vpc, etc. => create => we can go to stack and see events for all this happening => Go to ALB and see the url for it to access but that wont work because security group of load balancer is basic => change it to allow internet access => we can open url now which should work fine 
Go to cloudformation, select select, template, view in desginer => this will show visual graph of all the resources created and how they are related to each other.

AWS ELastic Beantalk
--------------------
AWS Elastic Beanstalk is a service that can automate the creation and management of web applications running on Amazon EC2. What it would do, is it will actually create the entire environment for you, i.e. creating ASG, creating Amazon EC2, a web application environment running on top of that like Node.js, load balancer, even RDS if we choose => Thus elasticbean us platform as service solution. Beanstalk supports Java, .net, PHP, Node.js, Python, Ruby, Go and Docker.
It is like cloudformation only but in this case, it creates infra only for web apps related things/services.
Working: See the slide 656 => We have to create the VPC, public subnets in diff AZ as show ourself and everything after that is taken care by elasticbean => Developer just uploads the source code zip file to elasticbean and it will create ASG across AZs, ec2 instances inside them, load balancer on top of that and running the zip code on ec2 instances after setting up node js. All this is called elasticbean env as shown.
If we want, we can tell elasticbean to instead of creating ec2 instances,  run docker containers on ECS and rest of things will be like above. 

Application contain the environments (prod dev etc and each of them having resources created by elasticbean so can call them elasticbean env), the configuration of those environments (like diff backend url for diff env) and the versions of our application code (version 1,2..). Typically, the code will be stored in Amazon S3 buckets as diff versions of app and we can choose any version for env that we want to deploy. Also we can deploy 2 types of deployments in env => web servers which is a web application that listens for and processes HTTP requests over port 80 OR worker for long running task which we dont want webapp to do. Usually workers are connected to SQS queue from where they process message after picking. So usually customer send request to webapp and if request need long running task, webapp add it to SQS queue and worker picks it else web app if small task, process it and return.

Lab: We will create the SQS thing we just discussed => Go to elastic bean => create web app => name, nodejs, confugure more options => presets => if we choose high availability, it will show load balancer etc below but we will stick to single instance => We can tell what all elasticbean should make like network, softwares, ec2 instances, load balancer, monitoring, database, etc => create app with defaults => should take 10 mins and clikc the urla dn you should see default node js app code => we can now click upload and deploy which will create new version and it will be our app instead of node js starter app => But go to environment on left, create new env, worker env, name, nodejs, create => can click the queue and it will show 2 queue one main and other dead letter queue => Delete both env nnow.

AWS Systems Manager
--------------------
Systems manager is used for managing resources such as Amazon EC2, secrets etc but we will discuss the Systems Manager Parameter Store part of it, also known as SSM Parameter Store. This service is used for storing secrets and configuration information, so it's essentially a hierarchical storage solution used for storing things such as passwords, database connection strings, and license codes and those are stored as parameter values (KV pair) as name suggest where values can be plaintext, ciphertext, or encrypted as well. Thus value can be referred by unique parameter key. Like told, we can also have hierarchy and thus can be used as a.b.c in application code (like scala config we used to use). There is no native rotation of the keys/passwords and thus must be done on perioidic basis by external lambda function. Ex- Our application wants to talk to RDS and thus application will connect with SSM and give key and get back that info like password and connect with RDS 

AWS Config service
-------------------
This is used for viewing and managing the configuration of your resources on AWS and is often used in compliance scenarios. So for compilance you want your data within country and thus instead of going to all resources and checking manually you can see config at one place like in gocd. 
Benfits: can evaluate our resource configurations against desired settings, can get a snapshot of the current configuration of your resources in your AWS account, can retrieve configurations of any resources that exist in your account, retrieve the historical configuration, receive notifications when resources are created, modified or deleted i.e. change in config and view the relationships between the resources in your account.
So we will have number of resources sending configuration information to AWS config which it will store in s3 on top of which we can also setup SNS notif etc, kick lambda function, etc. We can also use it with System Manager automation (which is another component of system manager we havent discussed) to automatically remediate any configurations that don't align with our standard. We can also setup some rules in AWS config => S3 bucket server-side encryption enabled to check buckets config have encryption enabled, restricted-ssh to ensure security groups in use dont allow ssh, rds-instance-public-access-check to see none of RDS are public, etc. 
If encryption is not enabled on one or more buckets, you can be alerted to that fact.

Lab: Open s3 and choose any bucket which has encryption disabeld in prop and then add tag in it classification:sensitive. Open AWS management config => It will show all the resources that we have in our account => rules, add rule, aws mamnaged rule, s3 server side encryption enable select, next, tags => add tag that we created in s3 so this apply to that bucket only => create => This rule will see it is not satisfied i.e. no compliant and it do encryption itself => to setup remediatoin, go to actions for rule, message remediation, automatic, enable s3 bucket encryption, bucketname, algorithm as AES256 => this encryption is done using system automation and thus system automation will need IAM role to allow this and thus go to IAM roles, select System Manager => create role and give s3 full access => name and create role. Come back to conf service =>  enter the ARN of the role created => create remediation => Now go to action of rules and do reevaluate since remediation is setup later.   

AWS Secrets Manager
--------------------
It is designed for storing your secrets like your database connection information or your passwords like SSM Parameter Store but some diff are there: Secrets Manager will store and rotate your secrets automatically without needing to create lambda func for this with most of services like RDS, redshift, documentDb. But for others might still have to use lambda.
See slide 673 for diff from SSM.

AWS OpsWorks
-------------
AWS OpsWorks is a configuration management service which is managed service based out on popular Chef and Puppet configuration management tools. So, if an organization is using Chef or Puppet and they're moving to the cloud and they want to retain some of their existing IP, they can do that with AWS OpsWorks. The updates managed by OpsWorks include patching, updating, backup, configuration, and compliance management as well. Usually SysOps admin is submitting some configuration updates to AWS OpsWorks and those updates will be pushed to instances using those config using Chef or puppet.

AWS Resource Access Manager
---------------------------
RAM is a service that enables you to share AWS resources within your account or across accounts like subnets, databases, resource groups with other accounts or with differen Orgnaization units in an AWS Organization. Resource shares will be created through the RAM console, RAM APIs, or the AWS CLI or SDK.
What can be shared ? See link - https://docs.aws.amazon.com/ram/latest/userguide/shareable.html

Lab: Share subnet across accounts => So from earlier, we have management account and producton account in our AWS organization and thus we will create public subnet in VPC of main account and share it with prod account. IMP thing to remember is that prod account cannot view or modify the things in this shared subnet that belongs to main account or other account main is sharing this subnet with. It can create its own resources in shared subnet and modify them only => 
Go to management account ec2 => secruity groups and see VPCPeer custom VPC we created earlier where we will launch our ec2 insnrtacne => edit inbound rules for this group => add new rule for custom ipv4, add 10.0.1.0/24 which CIDR block of subnet 1a in management account => create ec2, choose custom vpc for peer and subnet as 1a => choose vpc peer security group we edited above => launch instance
Go to AWS resource manager console => settings, sharing with AWS org => Click resource shares now, create, name, type as subnets, choose public 1a subnet => allow sharing with principals in thsi org only => select DCT prod account, create => Go to prod account now and create ec2 instance, select this vpc and subnet we used above and it will show up (shared) in brackets => new security group for this all ICMP, and 10.0.1.0/24 for this subnet => launch => Connect to ec2 instance in prod account we just created and from the insisde of it, ping the private IP of instance we created in main account and it should be able to.  