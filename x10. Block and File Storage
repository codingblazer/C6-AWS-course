There are 3 types of storages => block, object storage (like s3), and file storage. 

Block based storage system (EBS)
---------------------------------
Like external hard drive that's connected to a computer, then for your OS it's block based storage i.e. meaning OS is communicating at low level i.e. at block level i.e. blocks, sectors, tracks of that physical disk. Now in AWS this hard drive (EBS elastic block storage) might be connected to computer through network but it is communication that is at block level. For block storage, we can create volumes and those volumes can then be partitioned (like C drive, D drive on windows) with each having their diff volume size and file system. In this case, if it is over network, distance can't be too long since communication at block level and thus these have to be created within same AZ as EC2 which is restriction for EBS by AWS. 
Some points :
=> Normally so far in course we were using EBS with EC2 and thus OS, and everything runs from that EBS. 
=> EBS can exist independently without being attached to any instance or reaatached to some other instance/boot to instance. In case EBS was created as root volume i.e. for booting EC2, when EC2 terminate, volume is deleted automatically which can be changed as well. And non boot volume does not delete automatically.
=> Earlier in AWS we couldnt connect multiple instances to one EBS but there is new volume type which offers multiattach and also it works with Nitro system-based EC2 instances only. Also max 16 instances can be connected to this single EBS and EBS must be provisioned IOPS io1 volume and needless to say all isntances and EBS vol should be in same AZ.
=> EC2 instances mmust be in same AZ as EBS volume and cross connect is not possible. Usually we create copy and then use it in diff AZ.
=> One EC2 might have one or more EBS volumes.

VOLUME TYPES: 
See slide 400 => SSD is solid state disk which are high performance than normal hard disk and you can see IOPS (input output operations that disk can perform per second) i.e. can be used for light application like test dev or normal, throughput (amount of data it can transfer/sec), also they dont support multiattach and can be used to run OS i.e. boot. For provisioned SSD, size throuput and IOPS are more and thus for prod IO instensive this is the one + imp thing is that io1 and io2 supports multiattach feature we talked about.   
See slide 401 => EBS HDD (hard disk drive) are used for cost optimized solutions but they cant be used for boot, have low IOPS and througput and size, they are usually used for warehousing of data or minimal task like log processing. 

COPYING SHARING ENCRYPTION:
How to backup our data in EBS =>  we can create a snapshot of EBS volume. A snapshot is a point in time copy of the data on the volume. And it's actually stored in Amazon S3 and we can take multiple snapshots and each snapshot is incremental i.e. only captures the diff i.e. changes since previous snap that is present and thus these can be used to fully restore the volume. Now it is possible that over time you have lot of snapshots and thus they will have deleted data as well in them and thus if want to free up some space, we can delete all snapshot and create fresh one. Also we can then create a volume from a snapshot in another availability zone which can be used a backup or attached to another EC2.
Also snapshot can be used to create a AMI in AWS which can be used to create volume directly in any AZ. Now there are rules for conversion of volume to snapshot and snapshot to AMI and AMI attachment to EC2 => See slide 406, 407 (Note that since snapshot are in s3 which is not AZ specific service, they can be cross AZ as well + AMI is AWS account which can be shared publically or across AWS accounts like limnux AMI is public).

DATA LIFECYCLE MANAGER (DLM): 
DLM is a way that we can automate our backup, so it creates the snapshots and AMIs from our EBS volumes automatically for us by  automating the creation, retention, and deletion of EBS snapshots from volume in s3 and EBS backed AMIs from snapshots. We can do regular backups using this in other AZ and thus it will protect our data and also create AMIs and refresh them regularly, reduce your storage costs by automating the deletion of outdated backups, create disaster recovery backup policies that backup your data to another account.

AMI (Amazon machine image): 
It provides info to launch instance like:  one or more snapshot for EBS or template for instance store, launch permission for who can use this, and block device mapping which instructs which volume to attach to EC2 when it is launched.
They can be community AMIs free, AWS marketplace AMIs like firewall third party with price for license and our own AMIs.

RAID WITH EBS:

RAID is redundant array of independent disks. So we can take multiple disks and aggregate them together and is been used for long time. So we take these diff disks and create a RAID array out of them and reason being 2 use cases 1) performance 2) redundancy. Now RAID is not provided by AWS and we have to work with OS to use the disks in RAID manner ourself. Now in real world, we have a RAID controller which is physical card device you connect to your computer and disks are connected to this controller and things will work but since AWS not support anything like controller or even RAID, we have to do it at OS level ourself. 
Type of RAID: Raid0, raid1....and so on but for AWS we use raid0 and raid1 as they are recommended. 
Raid0 is type of raid which is used for performance use case i.e. in this raid, we attach our multiple EBS volume to strip the data i.e. block1 of data goes to EBS vol 1, block 2 to ebs2, block3 to ebs 1 => data is split in these 2 EBS and hence during read, we are reading from multiple EBS instead of all reads happening on one EBS and hence the performance => though risk is that if one EBS fail, we loose data. 
Raid 1 is type of raid used for redundancy in which the data is written to all the attached EBS and thus this ensures redundancy of data and protection against any failure of EBS. This is also called mirroring.

Lab: launch ec2 instance => choose ami as window server 2019 base => next -> in storage, you will see EBS already as root, you can uncheck delete on termination, choose size etc or add another volume or encrypt it, also note that for general purpose SSD we get IOPS according to size and thus as you change it, IOPS will change => launch => Go to ebs section and yuou will see volume coming up. 
Create another new EBS and select same AZ as EC2 => we can also give snapshot id but skip => create => now go to this volume => attach and select ec2 instance. We can connect to our windows machine locally and inside it initilize this EBS, create volumes like C/D drive out of it, etc which instructor showed.

Lab: select running Ec2 instance => actions -> image, create => it will have both volumes we created above => create => We can go to AMIs and see this image getting created => actually for image creation the OS will shut down and then snapshots will be taken and AMI will be created => this is because when OS is running, volumes are in use with some protected data and we cant snapshot everything. Go to permission of AMI => can make it public or share with AWS account => You can go to EBS snapshots and see the snapshots => We can go to AMIs and do launch or go to EC2 and create and select particular AMI we created => Do through EC2 and this time select some other AZ => this will work fine and we can connect to ec2 instance and see that data is also there like other EC2 => terminate the instances now.

Lab for EC2 Image builder: this is used to build, maintain and distrbute images => in AMIs => EC2 image buuilder => but first create roles needed before doing this => go to roles in IAM => Ec2 => Add 2 permission Ec2InstanceProfileForImageBuilder and AmazonSSMManagedInstanceCore => create role => go to IAM > ec2 image builder => create pipeline => schedule can be cron etc but select manual for now => create ne recipe (recipe is used to bake AMIs with softwares etc) => AMI, name, version= 1.0.0, managed, linux 2, linux 2 x86, build components (can create our own or select) => select amazon cloudwatch agent linux and update linux component => test component select 'simple boot linux test' => next => create new infra config =>  name, IAM role select the one we created, t2 micro, can select VPC/subnet but select no VPC => next > distrubution has info like region for AMI, target accounts, licenses, etc when you distrbubte this image => next, create pipeline => select, action, run pipeline => it should take around 30 mins to build this AMI (meanwhile you will see it bringing ec2, terminating them and then test ec2 etc to build AMI) =>  once done we shoudl find this AMI under 'My AMIs' => thus we can use it.

Lab Migrate EBS between AZs: We should have 2 EBS volumes in volumes in diff AZ => lets try to migrate the one in 1c AZ to 1b => select that volume, action, create snapshot, create => you should find it creating in snapshots panel => go to permissiona nd make this snapshot public => select snapshot, actions, copy => another region select. We can also directly create volume from this snapshot  but in diff AZ directly => snapshot select -> actions, create volume, select diff AZ => create and we should see it.

Lab DLM => select any volume in ebs => tags => add DLM:Weekly tag => DLM will pick the volumes based on these tags to take regular backups => Go to EBS lufecycle manager => target volume, select tag, description, role default, next => weekly, monday, retention can be how many snapshot to keep at anytime or age for which they should be retained => review and create 

Delete resources => delete DLM policy => delete AMIs after deregistering in actions => deleete snapshots => delete all EBS volumes.


EC2 Instance Store Volumes
---------------------------
Now usually we saw EBS with EC2 instance but we can also use instance store which is storage on host server (EC2 is VM machine running on physical host on AWS datacenter and this physical host server has storage which EC2 can use called instance store). Now unlike EBS which are connected to NIC over network to EC2, these are on host server and are very close to EC2 and thus highly performant. BUT THESE ARE EPHERMERAL i.e. data is not persistent on them and as soon as instance shut down, data is lost. We can use these to boot EC2 and thus, we can if using for boot, that EC2 instance can't be shut down and only restarted as shut down will remove OS as well. Why use them ? Because they are cost effective and high performant + they can be definitely used for use cases like temp data like cache, buffer etc where data loss if host shut down for any reason is tolerable. These are created using AMI tempaltes stored in S3.
Lastly, instance stores cannot be detached and reattached, so you can't detach it and move it to another EC2 instance or versa.


File-based storage systems (Elastic File System, EFS)
-------------------------------------------------------
This is a network attached storage meaning that your computer has a network interface attached connected to what's called a NAS server or a network attached storage server. That server has its own disk, so it has its own block-based storage system. So for NAS server it is a block storage but for your computer, it is file based storage since your OS is not communicating with storage directly. So we usually create a file system inside NAS server and then mount that file system onto our computer/EC2 instance. From EBS, it is diff that you can't format it here, you can't create volumes in it in this case + your OS is not communicating directly as hard drive is not connected to your NIC but the server is connected to NIC. All that's taking place over on the NAS server. All you can do is mount it and then read and write data. In this case, since this is file system over network it can be shared by many users over network to share files etc. 

This uses the NFS protocol, the Network File System for communication bw ec2 and efs. We create a mount point inside the ec2 instance and connect to efs. We can connect thousands of ec2 simulatenously to efs at a time. Note that this EFS is only for Linux and windows is not supported yet. We can connect to EFS in one VPC to ec2 in other VPC using VPC peering + to ec2 in other region + even to ec2 in another aws account + to ec2 in on-premises instances like corporate data center via VPN or direct connect. 

Lab: We will create 2 ec2 instance using the EFS => we should have 2 ec2 running in diff AZ => go to security groups => name efsecuritygroup, inbound rule, NFS type since ec2 will connect to efs using nfs protocol, select webacess securtiy group in source which is security grp of ec2 instance which will try to connect to efs => Go to EFS from AWS services => create, customize to see more details => name, regional for better fault tolerance and one zone for cost saving, backups regularly etc, lifecycle moves data to other storage class on unuse, general for normal IO and max IO for more IOPS, throuput we can specify exact value in provisioned, encrypt (IMP FOR EXAM) => we can set EFS to encrypt at time of creation only and there is no way to do it after it has been created i.e. recreate is only option => select security groups for only AZ where ec2 instances are => file system policy is used for avoiding root access or anonymous access (REmember the bame of this policy for exam) => creagte and it will take sometime => coppy the file system id i.e. dns name of efs => 
If we go to networks tab of file system, it shows mount target to which we can mount this EFS directly. Or we can copy the IP/DNS name of EFS and use that in our EC2 to communicate with this EFS. We will do it second way =>
ssh into your ec2 instance using connect => sudo mkdir efs (this will create directroty we will use for mounting) => 'sudo yum install -y amazon-efs-utils' => 'sudo mount -t efs <file system id which is dns name of EFS we created> efs/'  => Do the same on other ec2 instance and create file inside efs diretory in there and it should instantly come up in other ec2 instance too.

Amazon FSX: Amazon FSX provides fully managed third party file systems for your use => 
1. Amazon FSx for Windows File Server => this is for windows applications => it supports lot of features of windows file system like The SMB Protocol, Windows, NTFS File Systems, and Microsoft AD integration + file system specific features like Access Control Lists, shadow copies, and user quotas. Since managed, it has high availability, replications within AZs, managed Microsoft AD for authentication. 
We can connect to fsx not just from one AZ but from multiple AZ after enabling and thus our windows ec2 servers can connect from diff AZ to window fsx present in one AZ i.e. other AZ window ec2 server being called standby servers. Also, we can connect to window fsx through direct connect/VPN to on-premises corporate data centers. 
2) Amazon FSx for Lustre, which is for compute intensive workloads like high computing HPC workloads. machine learning, video processing etc. It works with s3 meaning that though this fsx is file system, it stores and read from s3 (IMP FOR EXAM) => Like for window fsx, we can connect it with on-premises center using vpn or direct connect. This on-opremises connection is used for clud bursting or data migration cases. 

Object based storage systems (S3)
----------------------------------
S3 we have a bucket and a bucket is the object storage container where objects are uploaded using HTTP GET PUT etc and we don't have a file system here i.e. we can mimic that by creating folders etc but it is all object and thus not actual file system like above cases. It is highly scalable unlike above since it is managed by AWS and cost effective. 

AWS Backup 
----------
This is a tool to create schedules to backup mutliple AWS resources like volumes, file systems, database => AWS backup => create => template => daily 35 day retention => Can also enable windows volume copy shadow service which takes consistent snapshots from the running applications. => create => in list you will see this plan and also one plan which was auto created for EFS we created => Click plan we created => assign resources =>  assign by resource id or tags or..,. => in settings you can see we can create cross account backups, backup policies as well. 

AWS Storage Gateway
-------------------
AWS Storage Gateway is a service that enables you to connect your on-premises storage to AWS. In our on-premises data center, we deploy a File Gateway, a Volume Gateway, or Tape Gateway depending on the use case and these storage gateway are deployed on virtual appliances or hardware appliance as well.

File Gateway => your data center servers are connecting to your storage gateway and mounting the file system inside AWS using the NFS or SMB. This file gateway which is on data center side has local cache to have low latency for recent used data. The files are stored as objects in s3 in multiple tiers or multiple storage classes of S3. The transit path of this is encrypted => So this is interesting because this means you are able to use file sytem benefits but underlying storage is s3 which is low cost.

Volume Gateway => cached volume mode and stored volume mode => In cached one, the cache of mosst recent data is stored on-premises not in local cache like file gateway but in actual storage and rest of data is stored in EBS which is backed up as snapshots to s3 and the protocol we use is iSCSI which is block storage protocol. In stored mode, the entire data is stored on on client and it is asynch backed up in s3 using snapshot out of ebs volume on AWS side. 


Tape Gateway => In this the backup also is created on on-site => then we connect this backup virtual tape to tape gateway which connect to s3 directly for it's Glacier or Glacier Deep Archive storage classes. In this case, for onsite backup we can use variety of backup applications on client side like NetBackup, Backup Exec, and Veeam. You have to choose the sizes of your virtual tapes on on-site and it can have up to 1500 virtual tapes with a maximum. Data transfer again happens in encrypted fashion using SSL/TSL.


