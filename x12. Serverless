Serverless is a concept in which you don't manage the underlying servers, which run the capability and we have serverless compute services like lambda, storage services like s3, queue services like SQS and databases like dynamoDB and these usually works in architecture driven by events. There is actually a server running all these internally but we wont manage it, AWS will i.e. no security management, patch management, backups, fault tolerant replication, EC2 servers manage, OS/software management, capacity provisioning for scaling. Also depending on the use case, they can very cheap. 

Lambda
-------
Lambda runs a function where we upload code and until that code runs, there is no charges => when code or test runs on it, it is billed per millisecond and for actual execution time + resources allocated =>  VIMP thing to note about lambda is that its max execution time can be 900 seconds, so 15 minutes and thus use it for use case like that only. Lambda easily integrates with other AWS services and event services like SNS to create triggers and act itself on based on triggers generated by other AWS services.
Some use cases are data processing, real time file processing, real time stream processing, and serverless backends for various different types of application.
Invocations of Lambda =>
 1) Synch using CLI, the SDK, or API gateway and the result is returned immediately in these + error handling must happen on the client side. ex - retrying if it doesn't work, then your application must be coded to retry.
2) Asynchronous invocation using S3, SNS, CloudWatch Events, and other services as well + in this case lanbda handles retries itself and retries 3 times + it is thus important that processing idempotent i.e. it should not be that first time it failed, it created some directories and second time it tried, those directories being present creates error => so handle it properly in code that retry can work.
3) Event source mapping using stream-based services like SQS, Kinesis Data Streams, and Dynamo DB streams + in this case lanbda does the polling on these streams for any changes and act based on that and they can also create triggers for lambda.

Note: If multiple invocations happen, same lambda will be running multiple functions in parallel and order is not gauranteed + there is limit on how many parallel functions can run based on burst which is initialized by us or account limit which concurrency quotas set on our account depending on which region.  If the limit for concurrency is exceeded, then you'll see this error message : rate exceeded error message and a 429 too many requests exception. (IMP for exam)

Lab for Lambda: We will create a lambda function and when it run it will also log the results in cloudwatch. Aws services => lambda => create function => author from scratch => name, node js, role is permissions for lambda and it dont have any by default => can be some existing role or basic lambda permission it need or general policy template based => choose basic lambda, and create => in this we can add event source and select any service like s3, etc and then there is destination where we can have asynch like SNS, etc and stream where we can put result in SQS queue etc => We will come back to these later. Go to code tab => we can upload code from zip or s3 or write it => In the test tab, we can run the code and it will simply execute whole lambda => we should see the log for the function in cloudwatch + this tab also shows time taken, billing time, memory taken etc. In monitor tab, we can see metrics for cloudwatch function, logs that will be sent to cloudwatch => we can do view cloudwatch logs and see them in cloudwatch => In configuration tab => permissions, you will see the role lambda has for writing logs to cloudwatch under basic lambda permissions we gave and this is how it is able to write the logs => alias is used to refer to specific lambda function and its version in the code. Version is diff versions of code we might upload in the lanbda over time.


Application integration services
---------------------------------
Often sit between different layers of your application:
=> Simple Queue service: SQS is a message queuing service and application component like a Lambda function or an EC2 instance has some kind of information that it puts into a message in a queue. That information is then stored in the queue until another application component comes along, reads the message from the queue and then processes whatever data is recorded inside. So it helps in building distrubuted and decoupled applications.
=> Simple Notification Service: sending notifications like text messages, emails, or other services like lambda.
=> Step Functions is a service that orchestrates or coordinates different components of an application in workflow manner.
=> Simple Workflow Service is in many ways, the older version of Step Functions. But for some use cases where there is external flow involved like human input which in one of step and not compltely automated, its still used there.
=> Amazon MQ is very much like the SQS service and is based on Apache Active MQ and RabbitMQ. This is for users already using Active/Rabit MQ and don't want to move to SQS.
=> Amazon Kinesis collects data from sources like iot devices, processes, and analyzes data.

Differences: Kinesis and SQS, processors/consumers are pulling the data from analyzed data and queue resp and if not pulled, it will expire, whereas with SNS it's pushing the data out to the subscribers.

With kinesis, we can as many processors and we can group the processors and stream data to each of them and set the throughput. We can get the ordering in this at shard level. In SQS however, there is no ordering gaurantee for standard queue but it can be done using FIFO queue. And lastly for SNS, its publisher subscriber model and can have 10 million subscribers with SNS and up to 100,000 topics.
Fan out architecture : Using SQS sending messages to multiple SNS topics which are sending messages to subscribers.

SQS
---
Using SQS we are able to decouple are MS architectures i.e. Normally in direct connection bw web app and server, we send requests and if server is receiving more requests than it can handle, it will start dropping requests leading to loss of data => we can decouple this direct connection and introduce SQS and all request will sit in SQS and server will take requests from it, as much as it can handle and requests are not dropped. 
There are different types of SQS queue: 
1. Standard queue and it offers best effort ordering i.e. not gauranteed => records might be processed out of order and thus if order is important, we can solve in 2 ways a) either put order string in messages and thus on server side use them to process in order b) Use FIFO queue. But since no ordering it has high throughput with unlimited transactions/sec.
Also in this queue, message is delivered atleast once meaning duplicate message might be send and you should be able to handle that on backend server or use FIFO which ensure no duplicate messages.

2. FIFO queue => first in will be first out for delivery and order is gauranteed => and because of this throughput is less 300 message operations/sec where operation can be like delete, send, receive and also use batch for each operation i.e. 10 message per batch for operation making 300*10 messages send/sec. FIFO has exactly one processing meaning it will never send duplicate messages to server. In FIFO queue, for ordering and ensuring no duplicates, we have to provide 2 fields in message:  Message group id : all messages having same group id will be processed in order by FIFO + message deduplication ID: this is used for deduplicating at duplicate messages. 

Dead letter queue configuration => this is not diff type of queue but config for aany of above queue where we ask to create additional queue with main queue where any message which is not getting processed by the consumer are dumped to by the consumer itself and thus we can come to this queue later and look at the message. With this, we can configure settings like name of queue which should be used as dead letter queue, redrive policy, and max num of receives before message goes to dead letter queue. 

Delay queue => in this queue, we add a certain delay along with the message and until that delay has passed that message wont be visible to processer/lambda and nothing can be done and when delay passes, message is made available to lambda for processing.

Polling of queue by consumer => 1) long polling => in this the server request for messages will wait for certain time for message to arrive and in short polling, the request for message is sent to queue, if found message is retunr and if not, it immediately without waiting returns.
With long polling we will make less API requests from consumer meaning less cost. Wait value can be between 0 to 20 sec for this and thus 0 value corresponds to short polling.

SNS (Simple notification Service)
----------------------------------
It's a publisher/subscriber system. So, we have a publisher and that publisher will produce info to send to an SNS topic and SNS from here will send that info to all subscriber of that topic via diff protocols it supports: HTTP, SFTP for mail, mobile push or an SMS text message. One topic can also make 2 deliveries to all subsriber like both email and message for OTP. Now the subscriber can be AWS service as well like lambda, SQS or webapps.


Lab for simple event driven app: We will send message to SQS from CLI and that messages will be picked by lambda and sent to cloudwatch and dynamo db as well. See file instructions.md in simple event driven folder => Open dynamo db => create table, tablename from intruc.md file, partition key which is primary key, create table => Open SQS service => create, give name, create => copy the queue url and paste in instructions.md file => AWS lambda, create function, name, nodes js 12, create new role from policy templates => Copy name of policy from file and add => create => code, upload zip DCTProduc.....zip inside the folder => 
Let see if SQS is working fine => go to command line and use the command in intruc.md by pasting your queue url and run the command from inside of that folder only => Go to SQS after that => click queue name => send and receive mesage, poll for messages => you should see the message => run same command but send message2 this time which is in same folder.
Go to SQS -> lambda triggers => configure => choose function to invoke which is lambda we created above => Now go back to SQS and again poll for message and you should see both messages are gone and they are actually gone in dynamo db => open dynamo db and see these inside your table. We have added the dynamo db sending from lambda inside the lambda code, thats how they are integrated. Also we can go to cloudwatch and see the logs being sent from lambda here.
Delete the queue + delete table in dynamo db + delete lambda function.

Step Functions 
--------------
AWS Step Functions is a service that coordinates the components of an application and we can add things like waits, checks on results, lambda for processing, SNS for notifications, etc. => used to build distributed applications as a series of steps in a visual workflow. See slide 516 as example and thus we can build this type of flow using the state machines which are written in JSON-based Amazon States language. We can then start executions and within the console we can actually see the real time status of each step within the visual workflow created using this language.

Lab for State machine: Go to AWS step functions, create, you will see definition and visual representaion for it => next, create => run the step and we are giving input value for hello world as true which is used in step => You can see the graph inspector as the steps are executed, they are becoming green > we can see the execution input and output tabs in it => 
Now lets see how to use this with lambda => See step function folder and md file for instructions => go to lambda, create, copy name from md file => node js 12, create new role and create =>  go to code tab and paste the codee in md file instead => copy the arn of this lambda function. Test this lambda by going to test tab and pasting the test input data in md file and it should work "Hello aws step function" output => Now create state machine => step functions -> state machines => write in code => paste code from md file => paste the arn of your lambda function in this code and create
Start execution of this state machine and paste the input from md file and it should show oputput 'Hello sachin'.

Amazon EventBridge
------------------
Amazon EventBridge is a serverless event bus. And it's useful for building distributed event-driven applications and used to be known as Amazon CloudWatch Event which acts on state change in any resource and triggers event and does action.
Event sources => AWS services, custom applications or some kind of third-party SAS-based application and any state changes are sent to eventbus and eventbus has rules which it applies to find out the destination (called eventbus targets) like Lambda functions, Kinesis data streams, or AWS Config. Ex- EC2 as event source and termination of it can be event in ec2 event source and this event is sent to event bus and based on rule, it sends it to SNS for sending email.

Lab: We will send event of ec2 termination to event bus and that rule will be processed and notif send to email using SNS. Start one ec2 instance and copy instance id. go to event bridge service => create rule, name, event pattern, predefined pattern by service, aws, ec2, ec2 state change notification, specific state, terminated, specific instance id and paste id => target, sns topic and choose already present topic 'emailnotification' => create rule => go to ec2 and terminate and you should see email.

Amazon API Gateway
-------------------
Amazon API Gateway is a service that you can use to create APIs and it supports rest APIs, HTTP APIs and web socket APIs. This API can then connect to the various backend services like Lambda, load balancers, etc. The API can do functions like modify the request in desired format for the backend, can support templates like swagger. It is usally called endpoint.
Types of deployment for APIs:
=> Edge-optimized endpoint => gives reduced latency for requests that are coming from large parts of the world because sitting behind Amazon CloudFront. and using CloudFront edge locations and CDN capabilities.
=> Regional endpoint => This is better if you know requests will not come from all over world but from certain region but can also be integrated with cdn. Endpoint api is itself deployed in that same region.
=> Private endpoint => This will be fully within your VPC, so it's a way of securely exposing your APIs only to services in the VPC or to applications that are connected via a Direct Connect link.

REST API
See slide 529 => in api, we can configure the method request, integration request, method response and integration response => In mthod request of our API we define the diff methods like GET, PUT, etc and these are mapped to integrations of integration request part. Now in integration request part, we have logic which modifies the request and its parameters in the format required by endpoint like lambda, ec2, etc. Then for response, we can have integration response where we can do conversions of our response payload and finally we have method response where we can change the status code, to format required by frontend. 
Lets see the example of integrations by looking what all integrations are possible when working with lambda => 1) Proxy integration which just passed the request to correct endpoint i.e. lambda 2) Custom integration where we have more control over request params.
Similarly for HTTP endpoint, there is proxy and custom integration and for AWS service, we only have non-proxy type i.e. custom only.

HTTP API 
In this instead of method request, we have routes. And the routes are also integrated into things like Lambda functions. And you'll see that in the hands-on shortly.

CACHING/THROTTLE IN API
Caching is a great for improving the performance of your API by provisioning an API Gateway cache and give size in gigabytes and it will cache the endpoints response thus reducing the number of calls to the backend endpoint and reduce latency for the requests to the API. This is great when you are paying per request on lambda/endpoint. We can also add throttle on our API because there are limits on lambda etc for parallel requests and thus we can add throttle on API layer, however we still have to handle it on application layer so that requests are resubmitted.

We can also use throttle for serving premium and general customers diferently i.e. in API , we can configure the throttle based on incoming request i.e. if request has API key of premium customer we will provide better performance to him through one path and for general customers, we will have less performant one on another integration path. 





