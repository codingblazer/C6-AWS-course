Relational DBs => schema, structure lang, tables, restriction on db side => Amazon RDS, Oracle, MySQL, DB2 & Postgres.
Non-relational DBs => flexible schemas, no structured lang, storage based on KV/graph/document, rules like constraints etc on application side rather on db side => DynamoDB, MongoDB, Redis, and Neo4j.

Transactional or online transaction processing (OLTP) DBs => these are both relational and non relational DBs that we use in production to store application data like RDS, Oracle, DB2, and MySQL, MongoDB, Cassandra, Neo4j or HBase.

Analytical or  Online Analytics Processing or OLAP databases => These are not operational ones and sits on back of transactional ones are used for analytics like customer behavior, and thus DB used for long transactions/operations. Ex Redshift, EMR for hadoop.

Deployment options
-------------------
We can first deploy our database server on normal ec2 server and this way we will have full control on scaling, OS and can use any DB not supported by AWS also.
OR
Use Amazon RDS which is managed relational database service for diff types of relational DB engines like Oracle, Microsoft SQL and MySQLmazon and DynamoDB which is for non relational and then we have redshift for data warehousing. ElastiCache is great for caching in memory for really high performance and often you're caching data from another database like RDS database.

AWS RDS
--------
RDS is a managed relational database and runs on EC2 instances which we can choose type for performance (database CPU, memeory, storage like EBS). It includes DB engines => Aurora, MySQL, MariaDB, Oracle, Microsoft SQL Server, and Postgres. We can scale these up either by scaling vertically where we change our instance type to bigger one or we can scale out (horizontal) but this increase performance in limited things like for onyl reads and also used for things like disaster recovery, fault tolerance.

Architecture for MySQL RDS: We have a RDS master which is our primary database where both reads and writes get performed. 

Standby replicas: We can then have a stand by instance where master will synchronously replicate to the standby instance (sync meaning at anytime both will be same because request will return response only when write to both are done) and this can be created in diff AZ for disaster recovery and thus this feature is called multi-az where standby DB in other AZ can automatically takeover as primary DB in case of disaster. 

Read replica: Read replicas use asynchronous replication, so some delay is there but they are also created from primary master only and since writes are asynch in read replicas, they are used for read queries and thus they are allowing us to scale horizontally but for specific read operations only. (Scaling write would have mean, we can send write to any one of replicas to distrubute load but thats not happening and is not even possible). So write always go to primary and from there sync to standby and asyn to read replicas. Read goes to primary and all read replicas in distributed manner.

Backups and Recovery; 
we can set a start time and also a duration for automatic backups which are taken as a snapshot and can be retained for zero to 35 days. If we use restore snapshot, it creates a new database instance. 
We can also use manual backups which also takes snapshot but key diff is manual + it takes backup of not just DB but its EC2 DB instance + these snapshots dont have 35 days upper limit and can be kept forever.
Also note that if backup is done, there is brief suspension of I/O i.e. access to your db during backup and thus it should be done only in maintainance window of your business which can be set as option. Or we can also use multi-AZ feature in which because of multiAZ, backup is taken from standby and no suspension. But there is one exception of  Multi-AZ SQL Server where because of its design, backup is not just taken from standby and thus slight suspension on master also is seen which can be negated by doing this during maintainance phase. 

Maintainance Windows: perating system and database patching does sometimes require taking the database offline and this is where you can specify your maintenance window and also set your own time and day let say every week. So all such tasks which might take DB offline are done during this week.

Security: We can make RDS public over internet or we can use security groups for RDS telling what all other security groups can access it and have a group for ec2 instances accessing it. We can enable SSL/TLS encryption so that we have encryption in transit for the connection from application. We can enable RDS encryption at rest as well using advanced encryption standard 256-bit encryption which will auto include read replicas, storage, snapshots of the DB encryption. We can't change i.e. enable/disable the encryption at rest once created the database. And that includes disabling encryption for databases. We can also use AES 256-bit encryption is a very strong encryption standard, but it has minimal performance impact as well on RDS +  on Oracle or SQL Server we can have transparent data encryption standard and that can have a performance impact as well.
IMP FOR EXAM:  Cannot have an encrypted read replica of an unencrypted primary or unencrypted read replica of encrypted primary i.e. both should have same encryption status. The same key will be used from KMS for all i.e. but if read replica in diff region, that region's KMS key is used. Same encryption applies for backups: can't do unencrypted backup or snapshot to an encrypted DB instance => only way to do this is by copying this unencrypted snapshot to encrypted and then use that to create new RDS which is encrypted.

Create RDS Mysql db => Amaazon rds => databases => create, standard, msql  =>  we will choose free teir but lets see options in production =>  
configuration options => name, master username as admin, enter a password, DB instance class on which db server will run, leave it on standard since no t2 micro, provisioned IOPS SSD io1 volume with 100 GiB for disk where db will write, can auto scale the storage layer and by default, that is enabled, Multi-AZ, which on this production template is also enabled by default, default VPC. For public access leave no since we dont want it to be open to intenet, see security group has port 3302 => make sure ec2 instance connecting to it also has security group entry for this => cost you can see that it is expensive, automated backups can be 0 or more, encryption using kms, enable performance insights, monitoring, maintainance you can see it can automatically do version upgrade and it will do so during maintainance window which you can specify, => scroll back and choose free teir => choose t2 micro here, see some of the options like encryption are gone => create db => click db and connetivity tab has url for db which we will use and port => monitoring etc tab normal and then backup tab shows retention etc backuop settings => Go to snapshots on left pane and you might see under system tab snapshot for this and we can go to manual tab to take manual snapshots => take snapshot =>  select db and done => click db now and actions => we can take snapshot from here as well + can take replica, restore etc. => Can use modify to change instance type and hence the scaling. 

Create read replica and multi AZ => go to digicloud db we created => actions => create read replica =>  keep the defaults like instance type for read replica etc + encryption do note that replica can be encrypted only if the primary db is also encrypted => so leave it and create replica. Once created we can see read replica has diff url and port => in our application, we can direct all the reads to this db and all the writes to primary db. We can also click our primary db and we can see the read replica being mentioned there i.e. both know of each other.
Go to primary db and modify => in multi AZ section, choose create standby => next => this warn that it might suspend IIO for while and if we want to do it now or do it in maintainance window. Now unlike replica, we dont see this multi AZ db in the list. After 10-15 min, if we go to primary and configuration tab => we will see multi az being true and also secondary AZ also mentioned.
Go to primary, actions, reboot, reboot with failover => this will move the primary db to standby db and thus once done you should see the AZ as standby db AZ for your primary and in configuration of this primary, the earlier primary AZ will become standby AZ now. 

Create encrypted copy of RDS => Since we cant change the encrytion status of RDS once created, we will take snapshot of its disk EBS and encrypt that and use this encrypted one to create new RdS => Go to snapshots, manual and select snapshot we created => actions => copy => defaults + choose encryption => copy => once created, select it => actions, restore snapshot to create new db out of it, give name, db instance class change to t3 micro => create and in db list we should see encrypted db which we can go to config and see the encryption key etc.
Delete all snapshots and all DBs. 

Amazon Aurora 
-------------
RDS Family => Developed by AWS themselves and great for moving off third-party databases that often have very expensive licensing and you will use DB of higher performance with low cost. Supports MySQL (5x faster than normal MySQL without Aurora) and Postgres engines (3x faster). It is distributed, managed and fault tolerant, and self-healing storage layer if any disaster happens and highly scalable to 128TB per instance. 

Architecture/Deployment options:
1. Aurora normal and within same region
See slide 571 => In case of aurora, we have a high performant self healing storage layer => Like mysql multi-AZ feature, here also we have multiple AZ and one of them has primary db and normal (forget about read replicas, diff concepts) replicas of it  in diff AZ (they must be in same region though). These are known as Aurora replicas.
Now we discussed about self healing storage layer which as shown, is spread across AZ and acts a single logical volume. When write happens, it comes to primary db and primary writes it to all copies inside the self healing layer. When read comes, it can come to any AZ nearby to the Aurora replicas and they just read from storage layer and return. So for both read write storage layer is used. Note that though we call it primary node or primary db, and similarly for aurora replicas, they are just servers for processing db requests wihout their own storage and any state and thus using logical vol layer for storage aspect. 
We get fault tolerance across three availability zones with a single logical volume. The Aurora replicas scale out read requests, and you can promote any Aurora replica to be the new primary (because primary does not have state like aurora replicas, logical vol has everything). We can have autoscaling in aurora and reads can be scaled by adding aurora replicas.
Features of Aurora : We get point in time recovery and continuous backup to Amazon S3, can work with MySQL and Postgres databases, autoscaling in form of read, failover taget if primary fails replica can take over, can have up to 15 Aurora replicas in region.
The storage layer or single logical volume internally has data copies across AZ and primary db makes write to storage layer and these writes are happening inside layer across AZ to every copy. So it provides fault tolderance across 3 AZ as in slide. Also note that each aurora replica is offering read capability only and have distinct urls for connecting to them that application has to know.

2. Aurora Deployment with cross region (only in mysql case)
This type of deployment gives cross region scaling and thus adds to fault tolerance but is only available with Mysql because the cross region replication are made using Mysql engine itself. In this, first replication happens bw regions by using mysql engine as told and this is called mysql read replicas (each region is replica region of primary region and each copied region is called mysql read replica) See SLIDE 576 => So each mysql read replica/copied region has aurora replicas we studied above. Thus in primary region, the primary db gets writes and it writes it to single logical volume layer of primary region and then uses sql to asynch write the same to other region's (or you can call these regions as mysql read replicas) logical vol layer. Now write is not scaled since it will always come to primary db but read can be done from multiple mysql replica regions because they have same logical vol layer as primary region and hence it is scaled for read across regions. It also has a failover target across regions now meaning that if primary region fail, the other region aurura node/replica will start acting as primary db. We can have upto 5 mysql replicas and for each of them 15 aurora replicas.

Diff bw mysql and aurora replica => Both are asynch replication but time is more in mysql replication. Also performance impact is less on aurora replication because mysql one involves sql itself. In failover event, the aurora replica can become master and there will be no data loss but in case of mysql, since asynch has more delay here, there might be some data loss if in failover you choose master from another region i.e. mysql replica. Also failover is automatic with aurora but not with mysql.

3. Global Database See slide 577
Cross region cluster with read scaling. So this works with both mysql and postrgres and can be used to scale read operation and have one writable vol layer only in one of the region which is primary region. 
In slide, we have primary region and we have reads and writes happening like normal case and then a secondary region which is created using asyunch replication of the storage layer itself and thus only reads are possible in secondary region and thats why this is real replication. where we just have reads and then we have asynchronous replication. The secondary layer aurora replica has reader endpoint for applications to connect to.

4. Multi Master Slide 578
Allows to scale out writes within a region using aurora replicas which can accept write operations and are each of them master. This is available for MySQL only. Cross region replicas are not possible and it can work in an active-active or an active passive mode. At any time, you can restart the database instances without impacting the other database instances.

5. Serverless Slide 579
Used for on demand scaling => In slide, we have database instances sitting front of database storage and we have a router fleet which takes the incoming connections from apps and as app demands more connections, it takes more nodes from the pool of capacity which AWS has available for it. Each unit in the pool that router fleet can assign to connection is 2gb memory + CPU and is called ACU (Aurora capacity unit). Thus this system brings the serverless concept because if there is no traffic we dont need db server/instance running and as traffic comes, we can pick running instances from pool and use them and thus it is scale based on performance. IMP use case is for infrequent used app or new application where scale is not known, variable workloads, unpredictable workloads where sudden spikes might come.
We have to connect directly through VPC or direct connect and not over internet. 

WHEN  NOT TO USE RDS
--------------------
1. Any time that you need a database type other than MySQL, MariaDB, SQL Server, Oracle or Postgres.
2. If you need root access to the operating system where the database is running.Ex - operational tools, management & security tools. But we do need to do backups, redundancy, patching, and scaling.
3. Cases for other services: If you need what we call lots of large binary objects or blobs => S3, Automated scalability for writes also => maybe DynamoDB is better.

Elastic CACHE
--------------
Amazon ElastiCache is an in-memory database and is often used for caching data that along with other database. It's full managed implementation of the popular Redis and Memcached database engines, a key valus store. Since cache, it has high perform and low latency and thus is used in front of other DBs like RDS to improve their perfomr and latency.
ElastiCache nodes run on Amazon EC2 instances. So, you do need to choose an instance type when you launch your database cluster. So first the application writes data always to RDS like in normal case and then RDS writes this data to elasticcache. But for reads, they come to elasticcache instead and if found, its a hit, otherwise miss and goes to RDS. 

Diff bw Redis and Memecache implementation => (IMP for exam) => See slide 587 => Note that redis can have modes i.e. cluster mode enabled or disabled as shown. Also, partition is done by creating shards out of all data where each shard contain part of the data. => In case of memcache, since sharding is possible, for horizontal scaling out, we add more nodes which makes less data per shart and same with redis cluster mode where we add more shards making less data per shard.

Use cases => Elasticcache is used where data is usually static and frequently accessed. Applications need to be tolerant of stale data because sometimes you might not have the synchronization between elasticcache and RDS(or any other DB). It can be also used to reduce cost since some DBs have their cost in terms of number of accesses and cache is reducing that. It is used for storing session state data, leaderboards, database caching, etc.

Scaling: 
1. Memecache => here we create a cluster which can be across AZs having multiple nodes, each having a partition of data. Thus it means that if node is lost, there is data loss as well. In this we can scale vertically by changing the node type and for this we must create new cluster with new node type and switch to this new cluster which has to be created manually (thus manual copying of data can be candidate for data loss, so have to be careful). Or to scale horizontally, we can add more nodes to the existing cluster.
2. Redis cluster mode enabled => In this also we have a cluster having shards which are eseentially logical grouping of nodes and there can be multiple shards and each shard can expand across moret than 1 AZ as well. Each shard contains a primary node, having partition of data but also a replica of data which was not there in memecache. There can 0 to 5 replicas in the shard. We can scale this in 2 ways => a) We can change the node type by migrating to new cluster and migration is automatic unlike memecache. b) Online resharding where we add or remove or upaghrade the shards c) Offline resharding in which we can do same as online resharding but also upgrade the engine and is more flexible to work with.
3. Redis cluster mode disabled => same as above, its just we cant have multiple shards in cluster and only one shard is allowed which will still be across AZ if you want and have primary and replicas. To scale this we can add replica to shard (scaling read this way) or we can change node type of the cluster which is automatic process for data migration. 

LAB: Elastic cache dashboard => new => If choose memecache, change node type to t2 micro, 3 nodes since each node is separate partition of data, select vpc and subnet from a,b,c => and can create BUT lets do redis instead => select redis, cluster mode enabled, it will ask number of shards and replicas per shard, and we will keep it off and now you will see numbr of shards go away,change ec2 to t2micro, choose replicas 2, leave defaults and create => Once created, we can clikc and see the node types, we can also select it, actions, modify and there we can change the node type, go to nodes and there select primary node, actions, and we can failover the node in which case replica will become primary and for replica node, actions, promote which will make it primary like above thing only. On cluster, actions, we can create backup and as well and see it in left pane of backup. Now go to cluster, actions, delete the cluster. 

DYNAMO DB
---------
AWS's serverless managed NoSQL database supporting key value store DS and document store DS. It has call push button scaling means that unlike RDS where on chnaging node type brings some downtime, here it happens instantly without any downtime. Though dynamo is DB, but the DB has tables and scaling happens at the table level and that too automatically based on attributes like read capacity unit or write capacity unit. The autoscaling here happens horizontally by AWS i.e. spreading the data across more nodes. 
In Dyanmo, we have tables, we have items (rows) in our tables, and then we have attributes (cells/fields in row) associated with our items. For every table we must have a partition key using which partition is done and then we can optionally have a sort key which is used to sort the items in the table by the sort key attribute and it may or maynot be present in items because its NOSQL and thus we can work in unpredictable structure unlike relational DB. So the sort key and parition key together works as a composite key and thus acts like primary key of Dynamo db.
In dynamo we can set time to live defining when an item in a table expires by putting a timestamp on it. This is very useful in session state store implementation. This feature comes at no extra cost and thus saves the cost because otherwise, we usually pay for read capacity unit, write capacity unit because when deleting, we have to first read the record to find it and delete the record which is write but here it is done automatically without these costs. This also reduces the amount of storage by manage the size of the table over longer periods of time.
For transactions, we get strongly consistent or eventually consistent transactions and support for ACID transactions as well. 

LAB: dynamo db => create table => See dynamo db cli command file in folder => name, parittion key, sort key => See the cpacity calc => read write capacity settings depend on your usage patter i.e. on demand means you will pay for what you use but provisioned means you researved somrhing and have to pay whether you used it or not. We can in case of provisioned have autoscaling for read and write mentioning target utilization when it should add unit (read or write) and min max read write cap unit. => we will choose on demand for this and create table => We can go to table details => indexes where we define seconday index other then primary field for fast access => Can choose backups and it will retain for 35 days => issue the command mentioned in folder file from inside the folder and go back to dynamo and do run and shuold see multiple items. We can also issue second command which can open all table data in json on your local command line for editing or viewing.

DynamoDB Streams: We have an application and a DynamoDB table and the application is going to insert, update or delete an item in the table and that record will go into DynamoDB stream on which Lambda function can then be triggered to let say write a record to CloudWatch logs.
So stream is time ordered sequence of item level modifications in a DynamoDB table and this is stored for 24 hours. We can choose what to store in stream i.e. key or specific attributes of modified item or New image which is item after modification or Old image which is item before modif or both images. 

DynamoDB Accelerator or DAX: It is a full managed high available IN-MEMORY CACHE service to improve the performance of your DynamoDB tables. Though Dynamo is already fast, this can improve performance from milliseconds to microseconds (THIS LINE IMP FOR EXAM). It can be both a read-through cache and a write-through cache so both read write performance can be improved. So to work with DAX, we can just put it in front of Dynamo without changing anything on application side. So requests first go to DAX cache and reduce latency if found else it goes to dynamo db. For security, we will need security groups for application to access DAX at 8001 and DAx to access dynamo at 8000 and then permission for DAX to access dynamo using IAM role.

DAX vs Elasticache comparison: Both are in-memory caching service but DAX is optimized for DynamoDB and thus DAX is best solution with dynamo as no complexity of application code handling, management overhead, invalidation of the cache. But elasticache supports more data stores, so as a positive for ElastiCache, it can sit in front of more databases, including DynamoDB.

DynamoDB Global tables: Global tables is where we can have our dynamo db in other regions than current region having separate masters (and thats why global tables are also called multi-master tables) and thus both read write being possible in all regions. The replication bw regions is asynchronous and is done using DynamoDB streams.
Note that each replica stores the same data and thus can be used as a failover.

LAB: Create Dynamo db global table to replicate data in another region => dynamodb, tables, our created table, global tables tab, note that it will automatically create dynamo db stream which is required to asynch keep replicas in sycn => create replica, choose diff region like us west and create => you can check both have same items and if create item in one, it will come up in second. Delete both the tables now.

AWS REDSHIFT
-------------
Redshift is a fast, fully managed data warehouse for analyzing data using SQL. It is relational DB and OLAP kind. Redshift uses Amazon EC2 instances and thus cost depeend on instance type. Maintain three copies of your data (replicas) + continuous and incremental backups. Being OLAP, as told earlier they work on back of OLTP like RDS and we might have multiple stores around the world, each with its own RDS database and pull that data centrally into redshift warehouse then run very long transactions and complex queries. Ex- Redshift, Hadoop with EMR.
Consider report generation as task and thus we should not run it on DB since it can effect DB performance for online transactions and thus 1) We can use read replicas which reporting tool can use and this way reporting tool is operating on live data as well 2) We can use redshift if let say data is coming from multiple RDS all around the world and not one DB. Once data is loaded to redshift from all diff types of DB, analytics can be performed. Also use redshift if we want managed data warehouse solution that offers automated provisioning configuration and patching, data durability and continuous backups to S3.
The sources of data for redshift can be from DynamoDB, Amazon EMR, Amazon S3, Data Pipeline, AWS Glue, Amazon EC2, and also on-premises servers. So there's lots of different ways you can
Redshift Spectrum => tool to use redshift with s3 or object type storage and allows you to run SQL queries directly on your data within S3.

Amazon EMR (Elastic Map Reduce)
-------------------------------
For analyzig large quantities of data. It's a managed cluster platform that simplifies running big data frameworks such as Apache Hadoop
and Apache Spark. It can be used for extract, transform and load (E-T-L) i.e. extract from some db, transform and load to another db.
EMR always runs within an availability zone and can attach to various different data stores S3, Glacier and Redshift and DynamoDB and RDS,HDFS.
EMR cluster has cluster instances for which we have root access and we can optionally have EBS vols as well. We can scale EMR by adding more cluster instances or launching more clusters itself.

AWS KINESIS
------------
See slide 623
Kinesis has Data Streams, Firehose, and Data Analytics (Video stream also but rarely comes). 
Kinesis data streams is the frontend where we ingesting our data from devices like IoT devices(called producers) and this data is stored in form of shards inside data stream for 24 hours by default. These data streams are real time (200 millisec) and data can be taken to process it with Kinesis Data Analytics(consumers) which uses flink for doing it and its all about running SQL. The data from analytics can then be streamed to AWS lambda then for custom manipulation maybe and finally loaded into dynamodb type thing.
Then we can also use Kinesis Firehose which is used to load data straight to destinations like S3 and data streams can be a source for firehose. Once in destination like S3, we can use Athena type service to do analytics. IMP diff in comparison to data streams is that this is near real time while data streams are completely real time.
In first flow, we could have also taken output stream of analytics and used firehose to take it to s3 as well thus using both firehose and analytics.

Kinesis Data Stream and Kinesis Client Library => This lib is part of the software architecture on the EC2 instances that helps to process data from data stream => See slide 625 => As you can see we can have this lib on Ec2 instance and then they act as a worker for processing data and load it to destination. KCL installs a agent and we can have one KCL agent per EC2 instance (or KCL worker). Now each agent then instantiate multiple record processors and each record processor is mapped to exactly one shard in data stream which it will process and vice versa as well i.e. any shard could be accessed by one record processor only. So, the number of shards can be greater than the number of instances you have to process them.
One IMP thing to remember for exam is that, the producers sends data to stream but internally it gets split into shards based on parition key but within the shard the order is maintained i.e. what went in first in the shard will come out first. But the order is not maintained across the shards. 

Data Firehose => Here also producers send data but there are no shards and it has advantage that its automated in terms of scalability, very elastic. With data streams, you have to add shards when you need to scale, monitor it continously and pay per shard BUT here no shards and automated scaling and this is also because unlike data streams, firehose never stores data but just forwards it to destination (though we can have lambda in bw) => As told earlier, disadvantage is 60 seconds latency which is near real time but not real time of 200 ms like data streams. (real time etc means latency in transfering data to destination). The supported destionations include Redshift, ElasticSearch, Amazon S3, Splunk, Datadog, MongoDB, New Relic, and also an HTTP endpoint as well and these are usually the destination where analytics can be performed. 

Data Analytics => Used when you want to perform real time SQL processing for the streaming data. It provides analytics for data coming from data streams or from Kinesis Data Firehose. The destinations can be Data Streams, Data Firehose or AWS Lambda.

AWS ATHENA AND AWS GLUE
------------------------
Glue is fully managed ETL service, extract, transform, and load i.e. preparing data for analytics and running ETL jobs on a fully managed scale-out Apache Spark environment. So Glue can read discover data sources like S3, redshift, rds, ec2 and uses crawler to populate glue's data catalog with tables metadata like schema etc of tables present in data source. Then we can create ETL jobs in Glue which will use catalog metadata to load data from source table and transform and send to destination table.  

Amazon Athena is a serverless analytics service that we can use to run SQL queries against data usually on S3 and data can be queried against diff formats like CSV, TSV, JSON, and Apache Parquet and ORC formats. Athena will have the tables and the data catalog or metadata for those are stored in AWS Glue which store information and schemas about the databases and tables that you're going to query and thus takes care of loading of tables in Athena part. Thus tables are built with help of this metadata from AWS glue in Athena telling it where to find what while querying/creating table in athena.
We can use other sources of data also but not directly but by using lambda in between acting maybe as a driver for those data sources like cloudwatch logs, document db, dynamo db, cloudwatch metrics, hbase, redshift, sql.

IMP FOR EXAM - HOW TO OPTIMIZE ATHENA => https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/

LAB: We will query s3 alb logs with athena => Create load balancer behind ec2 and go to load balancer => actions, edit attributes, enable acces logs, give s3 path for them, create this location for me => open load balancer url and hit multiple times from diff location to generate some logs => Can check s3 has some logs now in zip and we will use athena to query them directly => Go to Athena => get started, setup query result path where query results will be stored i.e. destination => which you can give as samebucket/queryResults => Create this path in s3 as well => Now in athena, paste the file from folder athena i.e. .sql file => in this file edit the bucket name to yours and account number and copy the content of file and paste in query => run query and you will see table on left for this => now in the file, copy seconf query to see first 100 logs => paste and run you will see results => run the other queries in the file as well 
Finally delete the table on the left, shut down ALB and ec2 behind it.
